# Global override values
global:
  storageClass: longhorn

# Clickhouse default values
# For complete list of configurations, check `values.yaml` of `clickhouse` chart.
# @ignored
clickhouse:
  # -- Clickhouse user
  user: admin
  # -- Clickhouse password
  password: 27ff0399-0d3a-4bd8-919d-17c2321e6fb9

  persistence:
    size: 1000Gi

  # -- Clickhouse cluster layout. (Experimental, use at own risk)
  # For a full list of options, see https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md
  # section on clusters and layouts.
  #
  layout:
    shardsCount: 1
    replicasCount: 1

# Default values for signoz
signoz:

  configVars:
    dotMetricsEnabled: true

# Default values for OtelCollector
otelCollector:
  # OtelCollector RBAC config
  clusterRole:
    create: true
    rules:
      # k8sattributes processor requires these permissions
      - apiGroups: [""]
        resources: ["pods", "namespaces", "nodes"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["replicasets"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["extensions"]
        resources: ["replicasets"]
        verbs: ["get", "list", "watch"]
      ## clusterMetrics
      - apiGroups: [""]
        resources: ["events", "namespaces", "namespaces/status", "nodes", "nodes/spec", "pods", "pods/status", "replicationcontrollers", "replicationcontrollers/status", "resourcequotas", "services" ]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["extensions"]
        resources: ["daemonsets", "deployments", "replicasets"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["batch"]
        resources: ["jobs", "cronjobs"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["autoscaling"]
        resources: ["horizontalpodautoscalers"]
        verbs: ["get", "list", "watch"]
      ## kubernetesEvents
      - apiGroups: ["events.k8s.io"]
        resources: ["events"]
        verbs: ["watch", "list"]
      ## prometheus
      - apiGroups: [""]
        resources: ["endpoints"]
        verbs: ["get", "watch", "list"]

  # -- Configurations for OtelCollector
  # @default -- See `values.yaml` for defaults
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            max_recv_msg_size_mib: 16
          http:
            endpoint: 0.0.0.0:4318
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
            # Uncomment to enable thift_company receiver.
            # You will also have set set enable it in `otelCollector.ports
            # thrift_compact:
            #   endpoint: 0.0.0.0:6831
      httplogreceiver/heroku:
        # endpoint specifies the network interface and port which will receive data
        endpoint: 0.0.0.0:8081
        source: heroku
      httplogreceiver/json:
        # endpoint specifies the network interface and port which will receive data
        endpoint: 0.0.0.0:8082
        source: json

      prometheus:
        config:
          scrape_configs:
            - job_name: 'nvidia-dcgm-exporter'
              scrape_interval: 30s
              static_configs:
                - targets: ['nvidia-dcgm-exporter.gpu-operator.svc:9400']
            - job_name: 'kube-state-metrics'
              scrape_interval: 30s
              static_configs:
                - targets: ['kube-state-metrics.monitoring.svc:8080']      

      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 30s
        metrics:
          k8s.node.condition:
            enabled: true

    processors:
      # Batch processor config.
      # ref: https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md
      batch:
        send_batch_size: 50000
        timeout: 1s
      # Memory Limiter processor.
      # If not set, will be overridden with values based on k8s resource limits.
      # ref: https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md
      # memory_limiter: null
      signozspanmetrics/delta:
        metrics_exporter: signozclickhousemetrics
        latency_histogram_buckets: [100us, 1ms, 2ms, 6ms, 10ms, 50ms, 100ms, 250ms, 500ms, 1000ms, 1400ms, 2000ms, 5s, 10s, 20s, 40s, 60s]
        dimensions_cache_size: 100000
        dimensions:
          - name: service.namespace
            default: default
          - name: deployment.environment
            default: default
          - name: signoz.collector.id
        aggregation_temporality: AGGREGATION_TEMPORALITY_DELTA

      # Default memory limiter configuration for the collector based on k8s resource limits.
      memory_limiter:
        # check_interval is the time between measurements of memory usage.
        check_interval: 5s
        # By default limit_mib is set to 80% of ".Values.resources.limits.memory"
        limit_percentage: 80
        # By default spike_limit_mib is set to 25% of ".Values.resources.limits.memory"
        spike_limit_percentage: 25

      resource/add_cluster_name:
        attributes:
          # k8s.cluster.name 추가
          - action: insert
            key: k8s.cluster.name
            value: dev

      resourcedetection:
        detectors: [system]

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      zpages:
        endpoint: localhost:55679
      pprof:
        endpoint: localhost:1777
    exporters:
      clickhousetraces:
        datasource: tcp://${env:CLICKHOUSE_USER}:${env:CLICKHOUSE_PASSWORD}@${env:CLICKHOUSE_HOST}:${env:CLICKHOUSE_PORT}/${env:CLICKHOUSE_TRACE_DATABASE}
        low_cardinal_exception_grouping: ${env:LOW_CARDINAL_EXCEPTION_GROUPING}
        use_new_schema: true
      signozclickhousemetrics:
        dsn: tcp://${env:CLICKHOUSE_USER}:${env:CLICKHOUSE_PASSWORD}@${env:CLICKHOUSE_HOST}:${env:CLICKHOUSE_PORT}/${env:CLICKHOUSE_DATABASE}
        timeout: 45s
      clickhouselogsexporter:
        dsn: tcp://${env:CLICKHOUSE_USER}:${env:CLICKHOUSE_PASSWORD}@${env:CLICKHOUSE_HOST}:${env:CLICKHOUSE_PORT}/${env:CLICKHOUSE_LOG_DATABASE}
        timeout: 10s
        use_new_schema: true
      metadataexporter:
        dsn: tcp://${env:CLICKHOUSE_USER}:${env:CLICKHOUSE_PASSWORD}@${env:CLICKHOUSE_HOST}:${env:CLICKHOUSE_PORT}/signoz_metadata
        timeout: 10s
        tenant_id: ${env:TENANT_ID}
        cache:
          provider: in_memory
    service:
      telemetry:
        logs:
          encoding: json
      extensions: [health_check, zpages, pprof]

      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [signozspanmetrics/delta, batch]
          exporters: [clickhousetraces, metadataexporter]
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [metadataexporter, signozclickhousemetrics]
        logs:
          receivers: [otlp, httplogreceiver/heroku, httplogreceiver/json]
          processors: [batch]
          exporters: [clickhouselogsexporter, metadataexporter]

        metrics/single:
          receivers: [prometheus, k8s_cluster]
          processors: 
            - memory_limiter
            - resourcedetection
            - resource/add_cluster_name
            - batch
          exporters: [metadataexporter, signozclickhousemetrics]
